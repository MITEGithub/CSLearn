## 四种机器学习系统

- 监督式学习

  > 回归 and 分类
  >
  > 在训练期间，机器学习从业者可以对模型进行预测时使用的配置和功能进行细微调整。例如，某些特征的预测能力比其他特征更高。因此，机器学习从业者可以选择模型在训练期间使用哪些特征。例如，假设天气数据集包含 `time_of_day` 作为特征。在这种情况下，机器学习从业者可以在训练期间添加或移除 `time_of_day`，以了解模型在使用该模型时是否具有更好的预测效果。

- 非监督式学习

  > [非监督式学习](https://developers.google.com/machine-learning/glossary?hl=zh-cn#unsupervised-machine-learning)模型通过接受不包含任何正确答案的数据进行预测。非监督式学习模型的目标是从数据中识别有意义的模式。换句话说，模型没有关于如何对每段数据进行分类的提示，而是必须推断自己的规则。
  >
  > 常用的非监督式学习模型采用一种称为[聚类](https://developers.google.com/machine-learning/glossary?hl=zh-cn#clustering)的技术。模型会查找划分自然分组的数据点

- 强化学习

  > [强化学习](https://developers.google.com/machine-learning/glossary?hl=zh-cn#reinforcement-learning-rl)模型通过基于在环境中执行的操作获取奖励或惩罚来进行预测。强化学习系统会生成[政策](https://developers.google.com/machine-learning/glossary?hl=zh-cn#policy)，用于定义获得最多奖励的最佳策略。
  >
  > 强化学习用于训练机器人执行任务（例如在房间内走动），以及训练 [AlphaGo](https://deepmind.com/research/case-studies/alphago-the-story-so-far) 等软件程序玩 Go 游戏。

- 生成式 AI

  > 生成式 AI 如何运作？概括来讲，生成模型会学习数据中的模式，目标是生成新的但类似的数据。生成模型如下所示：
  >
  > - 通过观察人们的行为和说话方式来学模仿他人的幽默搞笑者
  > - 通过学习大量特定风格的画作学习特定风格的艺术家
  > - 翻唱乐队，通过聆听某个乐队的大量音乐，学习听起来像一个特定乐队的音乐
  >
  > 为了生成独特的且富有创意的输出，生成模型最初是使用非监督式方法进行训练的，其中模型会学习模拟训练模型所用的数据。有时，模型会通过监督式学习或强化学习对模型可能需要执行的任务（例如总结一篇文章或编辑照片）相关的特定数据进行进一步训练。
  >
  > 生成式 AI 是一项快速发展的技术，人们在不断探索新的使用场景。例如，生成模型可以帮助企业自动移除干扰性背景或提高低分辨率图片的质量，从而优化其电子商务商品图片。

### 线性回归

### 平方损失函数：一种常用的损失函数

我们在此探讨的线性回归模型使用一种称为**平方损失函数**（也称为 **L2 损失**）的损失函数。单个样本的平方损失如下：

```
  = the square of the difference between the label and the prediction
  = (observation - prediction(x))2
  = (y - y')2
```

**均方误差** (**MSE**) 是指整个数据集中每个样本的平均平方损失。如需计算 MSE，请先计算各个样本的所有平方损失之和，然后除以样本数量：

其中：







### 迭代方法

### 梯度下降法 

> 在多维的矩阵中找到最快使损失函数收敛的多维度的向量

 

### 学习速率

### 随机梯度下降法

在梯度下降法中，**批次**是用于在单次训练迭代中计算梯度的一组样本。到目前为止，我们假定该批次是整个数据集。在 Google 的规模下，数据集通常包含数十亿甚至数千亿个样本。此外，Google 数据集通常包含大量特征。因此，一个批次可能非常庞大。超大的批量也可能会导致单次迭代就可能需要很长时间才能完成计算。

具有随机抽样样本的大型数据集可能包含冗余数据。事实上，随着批次大小的增加，冗余的可能性会越来越高。一些冗余可能有助于消除嘈杂的梯度，但与大批量相比，大量的批量往往具有更高的预测价值。

如果我们可以通过更少的计算量得出正确的平均梯度，会怎么样？通过从数据集内随机选择样本，我们可以从小得多的样本中估算出大的平均值（尽管会有噪声）。 **随机梯度下降法** (**SGD**)   将这种想法运用到极致，它每次迭代只使用一个样本（批次大小为 1）。如果有足够的迭代，SGD 可以正常工作，但噪声非常嘈杂。术语“随机”表示构成每个批次的一个样本是随机选择的。

**小批量随机梯度下降法**（**小批量 SGD**）是介于全批量迭代与 SGD 之间的折衷方案。小批次通常包含 10 到 1,000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的噪声数，但仍然比全批量更高效。

为简化说明，我们重点关注单个特征的梯度下降法。请放心，梯度下降法也适用于包含多个特征的特征集。

### 过度拟合

> 模型拟合得太接近数据集会出现过拟合，使得模型泛化太严重，只适合于一个数据集，不适合新的数据集
>
> 也可以说对新数据的适应能力不行
>
> 根本原因在于拟合过于复杂虽然理论分析在理想化假设下提供了正式的保证，但它们可能很难在实践中应用。机器学习速成课程侧重于实证评估，以判断模型泛化到新数据的能力。
>
> 机器学习模型旨在对新的、以前未见过的数据做出良好的预测。但是，如果您正在根据数据集构建模型，您将如何获得以前未见过的数据？好吧，一种方法是将数据集分为两个子集：
>
> - **训练集**——训练模型的子集。
> - **测试集**——测试模型的子集。
>
> 一般来说，测试集上的良好性能是新数据上良好性能的有用指标，假设：
>
> - 测试集足够大。
> - 您不会通过一遍又一遍地使用相同的测试集来作弊。

## ML 细则

以下三个基本假设指导概括：

- 我们从分布中随机**地独立且相同地**（**iid** ）抽取示例。换句话说，例子不会互相影响。 （另一种解释：独立同分布是指变量随机性的一种方式。）
- 分布是**平稳的**；也就是说，数据集中的分布不会改变。
- 我们从**同一分布的分区中抽取示例。**