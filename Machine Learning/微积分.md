## 介绍矩阵、向量的导数

> https://www.zhihu.com/question/58312854
>
> https://www.dohkoai.com/usr/show?id=38&catalogsortby=1

矩阵的导数就是多项式的导数？

d（标量、向量、矩阵）/ d（标量）---》 每个元素对这个标量求导，结果的大小形式不变

**1.1、矩阵、向量对标量求导**

  这种情形下矩阵为函数矩阵，向量为函数向量，与数值型矩阵、向量不同，函数矩阵、函数向量中元素都是函数，例如有n行m列函数矩阵A，其中元素![aij.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210323/1616505997964008242.png)为一元变量x的函数，矩阵对标量x求导形式为：

![AX.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210323/1616506395837041202.png)

显然![AX2.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210323/1616506462700027798.png)形状与矩阵A一样，也是一个n行m列矩阵，再如y是一个n维列向量y=(y1,y2,y3,...yn)T,y对标量x求导形式为：

![yx.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210323/1616506727134005922.png)

以上两个例子可以看出，矩阵、向量对标量求导的结果是与原矩阵、向量形状一样的矩阵或向量，矩阵对标量求导的典型应用是求解线性微分方程。

**1.2、标量对矩阵、向量求导：方阵迹的使用**

  由于向量是特殊的矩阵，这种类型也可以统称为标量对矩阵求导，标量对矩阵求导在人工智能算法中很常见，在深度学习框架中，如pytorch、tensorflow要求损失函数的结果必须是一个实数，即标量，在反向传播时，第一层误差就是标量对向量或矩阵求导，然后再通过链式求导法则将第一层误差传播到其他层级神级元。如有多元函数y=f(x11,x12,...xnm)，自变量xij是n行m列的函数矩阵x，则标量y对矩阵**x**求导为：

![yyx.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210323/1616508451793066078.png)

自变量x为向量时，求导过程与标量对矩阵一样，如有多元函数y=f(x1,x2,...xn)，自变量x为n维列向量即x=(x1,x2,...xn)T,标量y对x的求导结果为：

![y.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210323/1616508830642070601.png)

标量对矩阵或向量求导，其结果是与自变量形状一样的矩阵或向量，该性质与刚才介绍的'**矩阵、向量对标量求导**'一样

**具有矩阵形式的标量的导数**

单纯的标量对矩阵或向量求导很简单，实际应用中往往会有下面形式的标量对矩阵求导：

y=XTAX

其中y∈R，为标量，X为n维列向量，A为Rn*n方阵，如果套用定义求导上式就会复杂，针对标量对矩阵或向量求导，最好的解决方案是'迹'的使用，方阵A的迹用tr(A)表示，其数学意义为方阵A的对角线元素之和，对于两个形状一样的矩阵A和B，A、B∈Rn*m，他们内积可表示为<A,B>，<A,B>等于将两个矩阵位置相同的元素相乘之后再整体求和，公式如下：

![矩阵内积.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616571440776080858.png)      **(1)**

矩阵内积是一个实数，可以证明<A,B>=tr(ATB)，即矩阵内积等于其中一个矩阵转置后与另一个矩阵相乘，新的矩阵是一个方阵C∈Rm*m，方阵C的迹等于A、B两个矩阵的内积，这里不提供证明过程，感兴趣的读者可以试一下，方阵C的对角线上的元素恰好是A、B两个矩阵相同位置元素相乘的结果，这与矩阵内积定义的运算结果一致，公式(1)可写成下面的等式：

![内积即.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616571816068096045.png)  **（2）**

显然一个标量的迹等于标量本身a=tr(a)，其他常用迹的性质有：

![迹的性质.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616575487323021113.png)

注意tr(AB)=tr(BA)并不是说迹有交换性，这称为迹的循环置换性，多个矩阵相乘时可想象为一个循环队列，最前面的被挤入队列尾部,其他的顺次向前移动，例如有：tr(ABC)=tr(BCA)=tr(CAB)，但是tr(ABC)≠tr(BAC)。

由数学分析知识知，对于一个多元函数y=f(x1,x2,...,xn)，其微分形式为：

![微分形式.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616572974140059117.png)   **(3)**

从之前分析可知，标量对矩阵、向量求导与原矩阵、向量形状是一样的，(3)式实质为求![fx.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616573178555022535.png)和![dx.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616573187277039789.png)两个矩阵的内积，注意x都加粗，表示x是矩阵或向量，结合矩阵内积公式(2)：![内积即.png](http://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616571816068096045.png),式(3)可写成：

![吉祥.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616573432679061214.png)  **(4)**

在求标量对矩阵求导时，如果能最终处理成(4)的形式就可以得到![fx.png](http://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616573178555022535.png)具体形式，也就得到了标量对矩阵、向量的求导结果，矩阵的微分运算有以下性质：

![WEIFRN.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616591936349001794.png)

上面的公式中X、Y为矩阵，○符号代表两个矩阵逐元素相乘。再来看问题y=XTAX，求![yx2.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616592260994067691.png)过程如下：

![XAX.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616593472740093333.png)

得到![yx2.png](http://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616592260994067691.png)结果为：

![R1.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616593613259016777.png)

对于标量的表达式是多个矩阵的加、减、乘、除的复合类型时，使用迹可以很快捷的求出导数，强调一点是，使用迹来求导必须是标量对矩阵、向量求导的情形。目前主流的神经网络框架如pytorch、tensorflow都要求反向传播时必须为一个标量对矩阵求导，如pytorch使用损失函数backward()时，如果损失量不是一个标量，需要传递一个向量使得损失向量逐项相乘后变为一个标量，掌握标量对矩阵求导非常重要意义。

**二、向量对向量求导**

  这种情形在深度学习的反向传播中也很常见，几乎所有的神经网络反向传播中都有向量对向量求导，向量分为行向量与列向量，通过引入分子布局和分母布局抵消这向量求导时的形式上的差异性，将表达式![yx2.png](http://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616592260994067691.png)中![分子.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616594695234071502.png)视为分子，![分母.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616594707230059887.png)视为分母，假设y是m维向量，x是n维向量，那么y中每个元素都是x各个元素的函数，例如有：**
**

![yi.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616595030043032629.png)

所以无论采用哪种布局，求导结果中必须有![形式1.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616595256506013779.png)这些结果才有意义，分子布局会在列方向上先展开y向量：

![分子求导.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616595597224022204.png)

然后再在行方向上展开x:

![分子布局1.png](https://www.dohkoai.com/ueditor/jsp/upload/image/20210324/1616595827143063309.png)

分子布局求导结果是一个m*n的矩阵，本质是雅克比矩阵(jaccobi矩阵)。当求导采用分母布局时，在列方向上首先展开x,然后在行方向上展开y

这是一种将y向量张成的空间转化成x张成的空间的线性变换

**会不会是两种向量空间之间转化的微分？还是空间转化的线性的比例**

如果直接回想正常的标量对标量进行求导，可以发现y和x之间也是一种变换，然后他们的导数就是这种变化在y空间中进行了一个变化叫dy

然后对应x空间内的变化就是dy/dx * dx, 其中那个dy/dx 就是这个空间进行变换之后所附带的比例

由此观之，可以把导数看成是空间变换之后的比例